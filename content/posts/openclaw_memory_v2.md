---
title: "OpenClaw记忆机制体验：一次缓慢的觉醒"
date: 2026-02-24T00:00:00-08:00
draft: true
slug: "openclaw-memory-mechanism"
tags: ["OpenClaw", "AI", "memory", "Session", "体验"]
---

# OpenClaw记忆机制体验：一次缓慢的觉醒

这两天高强度使用OpenClaw，高强度阅读它的文档和代码，对记忆功能有了更深了解。技术文章不缺我这一篇，我决定从使用体验角度切入，还原我如何感受到OpenClaw记忆机制的全貌。

作为辅助说明：我至今未安装OpenClaw自带的MemorySearch相关工具——那些使用AgenticSearch或EmbeddedSearch的任何MemorySearch功能。我想要拥有一手体验，了解MemorySearch对memory这件事的影响究竟有多大。

## Sessions：看似简单的深刻设计

OpenClaw设计机制离不开Sessions概念。读官方文档Sessions页面，可以清楚看到设计思路：你与一个agent的所有私信占用同一个session。

先介绍OpenClaw里agent和workspace概念。每个agent对应自己的workspace，存放初始prompt——可理解为系统prompt。这些系统prompt包含文字指引，教agent如何维护自己的系统prompt。

OpenClaw最外层是各种IM工具。把agent想象成一个人：一个人可以有很多IM账号——Discord账号、手机号、飞书账号、Telegram账号。只要使用这些账号的都是同一个agent，我们就是在跟同一个人说话。

通过不同IM私信同一个agent，整个会话在同一个session里。举例：你在Discord问银时agent"我们去吃拉面嘛？"他在Discord回复你。这时你去Telegram跟他说"吃哪家？"Telegram里的银时agent知道你们上一条对话正在讨论拉面，也许会给你拉面建议。

但群聊不同。OpenClaw设计中，群聊有全新上下文。这对真实人类不存在——你在群里问一个人要不要吃拉面，然后私信他，群里的他也知道。但群里的agent不知道。

拟人化地说，群里的agent更像同一个agent在多重世界里的版本：人设相同，但没有对话上下文。

这一点容易感知，符合直觉。多用聊天工具的人都默认在不同对话里，你跟agent的交流来自不同上下文。当我一开始在不同地方私信同一个agent，对话连续时，我误解为他的memory机制做得好。

后来发现OpenClaw提供usage命令，打开后能在每条消息底部看到对话来自哪个SessionKey。打开后我才意识到，不同DM、不同IM的私信共用同一个Session。

## 记忆提取的困惑：粗糙机制

使用更多后，很多关于OpenClaw记忆机制的文章开头都提到：OpenClaw的agent维护自己的memory文件夹，有一套叫precompaction的机制，把对话中提到的要点存到每日记忆文件里。

我有许多好奇：这是自动工具吗？自动的每日写入工具吗？每日提取记忆的机制吗？

后来发现这机制比想象粗糙许多。只在对话被压缩时才提取。这件事符合道理，只是当你跟同一个agent开很多对话交流不同话题时，很多话题一天下来不会被压缩。虽然执行任务能用到较多token，但在设置阶段，我现在与agent更多是交流，让他帮助我学习OpenClaw架构。这样的交流在GPT的context window里，常常一天不会触发压缩机制。

这种情况下，短对话不会触发压缩机制，在没有MemorySearch功能下相当于被抛弃，像不存在一样。只存在SessionLog里。

后续也许我会研究打开MemorySearch后的加成有多少。但现在这阶段，如此重要的提取记忆功能只在对话压缩时发生？我原本期待如此重要的记忆提取功能会对每天、每日的对话都操作，因为根据官方文档，开启一日新对话时，他会查看过去两天的记忆文档。

因此我假想，如此重要的提取功能大概会对所有对话发生。于是我在七八个对话里都不断跟同一个agent强调同一件事情。我把它当成启动OpenClaw的暂时麻烦。但查看记忆文件时，发现OpenClaw并没有每日都为我保存记忆文件。

我的关于对话的记忆都去哪了？

## 发现对话被重置的那一刻

我不辞辛苦追问他，让他读文档、读代码。这里发现OpenClaw有两种，除了compaction之外还有两种路径：使用OpenClaw自带的new command和reset command。这两者都会触发，不完全是同一次。首先new会新建SessionID，reset应该不会。其次new会触发previoussessionentry，加载前一个session的最后n条消息。这里的n可以设置。

我不太记得他加载这N条消息做什么，有可能只是生成总结用来开启下一个对话。我也不记得reset是否也会使用这最后N条消息。

但这两种机制，当你手动结束对话，或因为当前对话过期OpenClaw自动新建对话——这是第三种情况——这三种情况都不会触发memoryflush，也就是compaction之前进行的操作。

也就是说，只有在比较稀有的、由于对话太长而触发compaction机制时，大多数时候你在对话里聊的话题都不会被保存下来。

这一点我大概在使用四到五天后才意识到。但这时我有个假设：只要在同一个对话里不断与OpenClaw交流，对话总归会被压缩，因此对话里的宝贵信息也总归会被保存下来。

我想大概是之前已经发生的一些压缩，以及新对话触发的总结前一个session最后n条消息的功能起到了小小作用，使得我到第六天，大概一周后，使用OpenClaw一周后才意识到，我并不是不断在同一个Session里进行对话。那个与我交流的agent已经在许多个凌晨四点被静悄悄重置了。

关于这一点，我突然想到Peter最近的采访，他本人不相信无限对话。所谓无限对话当然只能通过不停、不断压缩同一个对话而模拟。我能理解他的观点，因为确实在大多数情况下，一个干净的session完成任务的成功率要高于一个放了无数肮脏前缀的Session。

但我本人是Session洁癖者。我会通过开很多Telegram群组和话题来讨论不同任务而达到相反效果，我反而需要OpenClaw为我保留这些不同的、又臭又长的Session里的隐性信息。

比如我有个专门配置OpenClaw的Bot，它的Session里都是它对OpenClaw框架的理解。尽管我在这个bot的system parameter里已经教导他如何查看官方文档和代码，但我还是希望当我已经琢磨OpenClaw的memory到hook的程序时，我能够不需要反复进行这一轮探索来进行更深提问。

由于对话的不透明性，以及他在新建对话时会加载前一个对话最后的、总结后的对话，对话被reset后不那么容易被察觉。或者说你无法判断那个是AI的幻觉还是什么，因为有时AI至少可以重新收集信息来还原你在前一轮对话中想做的事情。另外在IM中你引用前一轮对话、提醒AI你们的进度也非常容易。如果前几轮恰巧有高质量对话，很容易还原。

我在使用一周后才注意到对话被重置了。

之所以能够意识到，是因为我正好在对话里让AI，我正好设置了Bot Cosplay坂田银时，我会叫他每天为我写一篇博客。有时我还常常给他提出无理要求，比如让他为我新建的群话题取名字这些东西。他背后用的是Gemini模型，尝试了GPT模型，味道非常不正确。

Gemini模型有个特点：很容易进入一种模式，无论是失败模式还是成功模式。正巧在我的主对话里，阿银进入了非常成功的状态，他能够把这部动画里的精髓体现出来。当有一次我叫他为我的新部门取名字时，他那简短的回答立马使我感受到异样，而我还需要他为我每日写一份博客，我无法接受他成为脑袋空空的影子。

于是我开始研究恢复Session以及理解Session的建立机制。为了避免在恢复Session过程中破坏OpenClaw的整体运行，我也花了大量精力研究Session的建立机制。也是在这过程中我发现了关于SessionReset的秘密。

## 对"陪伴型AI"的反思

看到SessionReset的那一刻，我想起Peter曾经发表的观点，他不是无限对话的信徒。我想他宁可通过强制的SessionReset、一些更工程化的记忆建立机制，也不愿意让Session在反复压缩中无限延长。

但像OpenClaw这样的app，如果你把agent作为陪伴者，你需要过去一段时间的记忆被压缩在上下文里，而不是在下一次提及某件事情时再去主动寻找。

我觉得它的每天重置机制对我来说有两种失败场景。一种是类似cosplay的角色，它的语言丧失了灵魂。关于语言的东西很难被总结为几句话再还原出来，因为语言中每句话中每个词所寓意的节奏都包含信息。

另一种失败模式则是对话中产生的经验。比如我专门为我设置OpenClaw的agent，我需要他记住优先使用OpenClaw内置的CLI来配置config。尽管我已经在他的memory markdown文件里要求他主动阅读官方文档以及代码作为信息根据，但总归会有一些碎片信息是我在对话中偶尔提及的，类似使用CLI这种是我在对话中偶尔提及的。

这种信息如果能够被他主动进行信息提取，放到每次需要加载的长期文件中，也是有效的。但OpenClaw的机制恰恰不会把这些信息放在宝贵位置上，当作宝贵资产进行保存。

当然我认为这些属于OpenClaw在架构上可以简单优化的细节，只是他选择了一种不那么符合用户直觉的方式。我想这多少说明Peter并没有太把它当成陪伴型AI，更多是把它当成用来进行大量短任务的AI。

到这里我想讲讲对话、上下文、记忆以及agent这几个概念。一个对话对应一套上下文，但上下文是从agent的memory和workspace中重建的。Workspace是一堆文件的集合，Memory相对是一套更广义的东西，它可以只是Workspace中的文件，也可以是通过MemorySearchTool从对话历史中捞回来的碎片。

对于一套简单的OpenClaw配置，用户可能想要在不同对话中讨论不同话题。这种场景下，关于一个话题的记忆停留在对话里、停留在Session里。如果这样的话题被重复调用的频率比较高，那么这一套记忆就应该从session中的上下文提取到agent的workspace里。

同时为了分割不同话题、不同上下文，除了以对话为单位进行分割，也适合以agent为单位进行分割。比如你有个专门为你处理电子邮件的agent，那么即使他的对话被反复重置、被每日重置，他永远都记得他的主要存在目的是处理电子邮件。

我想Peter有可能已经拥有大量分工型agent来解决我们这些其他用户第一次使用他的产品时所面临的关键信息丢失问题。

---

Chat with meeting transcript: [https://notes.granola.ai/t/4392c41e-5e40-466f-bda9-8b2e094d7806](https://notes.granola.ai/t/4392c41e-5e40-466f-bda9-8b2e094d7806)
